{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All models discussed in the paper are the same\n",
    "\n",
    "This implementation is using modelV3.py as the final revision of the model. V1 and V2 are also included with the code and are discussed in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Lots of imports here, by section they are...\n",
    "\n",
    "1. General imports, mainly for logging/reading\n",
    "2. Pytorch imports for the model and its training/validation/testing\n",
    "3. Imports from custom python files, such as the pytorch model, data processing, and dataset structuring\n",
    "4. Matplotlib for plotting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from modelV3 import Siamese_lstm\n",
    "from process import get_embedding, save_embed, load_embed\n",
    "from process import data_process\n",
    "from dataset import vocDS, VocabMaker\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.full_load(open(\"config.yaml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. Reads the dataset in its entirety\n",
    "2. Cleans the sentences of any puctuation and puts all letters into lower case\n",
    "3. Drops all numbering data from the dataframe, leaving only questions and labels\n",
    "4. Splits the data 80:20 for training and testing\n",
    "5. Based on inputs, grabs selected values from training and test set (This is because trianing on all ~430,000 question pairs wasn't feasible in the given time)\n",
    "\n",
    "\n",
    "Note: The values within this function were 60,000 and 6,000 in the baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Preprocess Data\"\"\"\n",
    "data_process(100000, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Reading the Data written by the data_process function \"\"\"\n",
    "\n",
    "train_data = pd.read_csv('ctr.csv') #len 100000\n",
    "test_data = pd.read_csv('ct.csv') #len 20000\n",
    "\n",
    "# split dataset\n",
    "msk = np.random.rand(len(train_data)) < 0.8\n",
    "train = train_data[msk]\n",
    "valid = train_data[~msk]\n",
    "\n",
    "#getting all sentences to put into the vocab\n",
    "all_sents = train_data['question1'].tolist() + train_data['question2'].tolist() + test_data['question1'].tolist() + test_data['question2'].tolist()\n",
    "all_sents_test = (train_data['question1'].tolist() + train_data['question2'].tolist() + test_data['question1'].tolist() + \n",
    "            test_data['question2'].tolist() + test_data['question1'].tolist() + test_data['question2'].tolist())\n",
    "\n",
    "#creates datasets with vocabulary\n",
    "trainDS = vocDS(train, all_sents)\n",
    "validDS = vocDS(valid, all_sents)\n",
    "testDS = vocDS(test_data, all_sents_test)\n",
    "\n",
    "#values should correspond with inputs to data_process above\n",
    "print ('Testing data size', train_data.shape[0], test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "When the embedding is created it checks the pretrained embedding for each word in the vocab, if a vector for that word exists, it takes it, else it creates a new vector to assign to that word. During this process it also makes note of the percentage of words from the vocab are not covered by the pretrained embedding. The presaved embedding had an 82% vocab coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['model']['embedding_saved'] = not config['model']['embedding_saved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['model']['embedding_saved']:\n",
    "    print(\"Creating embedding\")\n",
    "    full_embed_path = 'glove.6B.300d.txt'\n",
    "    embed_dict = get_embedding(trainDS.vocab.id2word, full_embed_path)\n",
    "\n",
    "    save_embed(embed_dict, 'embedding.txt')\n",
    "    config['model']['embedding_saved'] = not config['model']['embedding_saved']\n",
    "    \n",
    "else:\n",
    "    embed_dict = load_embed('embedding.txt')\n",
    "    print(\"Loaded embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(embed_dict)\n",
    "# initialize nn embedding\n",
    "embedding = nn.Embedding(vocab_size, 300)\n",
    "embed_list = []\n",
    "for word in trainDS.vocab.id2word:\n",
    "    embed_list.append(embed_dict[word])\n",
    "weight_matrix = np.array(embed_list)\n",
    "# pass weights to nn embedding\n",
    "embedding.weight = nn.Parameter(torch.from_numpy(weight_matrix).type(torch.FloatTensor), requires_grad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "### Siamese LSTM Model\n",
    "\n",
    "This implementation is a single layer lstm (due to computational bottlenecks) which will take as input two sentences. For each sentence it will create an encoding, and perform some operations on the two encodings and concatenate them to produce a long tensor. The tensor is then passed into a sequential double linear model as its classifier, which will take the now long tensor and get a length two sensor out as the model's prediction (To get a numerical prediction between 0 and 1, this tensor will be passed into a Softmax function).\n",
    "\n",
    "\n",
    "#### Main Hyperparameters\n",
    "1. Embedding size = 300\n",
    "2. Hidden size = 50\n",
    "3. Learning rate .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model time \n",
    "siamese = Siamese_lstm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = config['training']['learning_rate']\n",
    "optimizer = torch.optim.SGD(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log stuff\n",
    "train_log_string = '%s :: Epoch %i :: Iter %i / %i :: train loss: %0.4f'\n",
    "valid_log_string = '%s :: Epoch %i :: valid loss: %0.4f\\n'\n",
    "\n",
    "loadBest = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadBest:\n",
    "    if os.path.exists('Sgd4Best.pt'):\n",
    "        print('Loading checkpoint: %s' % 'Sgd4Best.pt')\n",
    "        ckpt = torch.load('Sgd4Best.pt')\n",
    "        epoch = ckpt['epoch']\n",
    "        siamese.load_state_dict(ckpt['siamese'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "        loss_states = ckpt['loss_states']\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "else:\n",
    "    if os.path.exists('Sgd4Long.pt'):\n",
    "        print('Loading checkpoint: %s' % 'Sgd4Long.pt')\n",
    "        ckpt = torch.load('Sgd4Long.pt')\n",
    "        epoch = ckpt['epoch']\n",
    "        siamese.load_state_dict(ckpt['siamese'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "        loss_states = ckpt['loss_states']\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "    else:\n",
    "        epoch = 0\n",
    "        loss_states = []\n",
    "        print ('Fresh start!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "### For each epoch\n",
    "\n",
    "1. Load our training dataset into torch DataLoader (each epoch the data is randomly ordered to prevent the model learning a patern)\n",
    "2. For each question pair in the dataset we first get the words and labels\n",
    "3. Clear the gradients\n",
    "4. Pass our sentences through our Siamese Model\n",
    "5. Compute the loss based on the label and the prediction\n",
    "6. BackPropogate\n",
    "7. Record loss states every 1000 or so iterations\n",
    "\n",
    "## Validation\n",
    "\n",
    "Same as above without the BackPropogation\n",
    "\n",
    "## Saving Model\n",
    "\n",
    "If the overall loss of a new state is less than the current record, save the model to thr Best save point, else save it to the Long save point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Training\"\"\"\n",
    "    \n",
    "# save every epoch for visualization\n",
    "train_loss_record = []\n",
    "valid_loss_record = []\n",
    "best_record = 0.3809\n",
    "\n",
    "\n",
    "while epoch < 40:\n",
    "\n",
    "    print ('Start Epoch{} Training...'.format(epoch))\n",
    "\n",
    "    # loss\n",
    "    train_loss = []\n",
    "    train_loss_sum = []\n",
    "    # dataloader\n",
    "    train_dataloader = DataLoader(dataset=trainDS, shuffle=True, num_workers=2, batch_size=1)\n",
    "\n",
    "    for idx, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "        # get data\n",
    "        s1, s2, label = data\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # input\n",
    "        output = siamese(s1, s2)\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "        # loss backward\n",
    "        loss = criterion(output, Variable(label))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.cpu())\n",
    "        train_loss_sum.append(loss.data.cpu())\n",
    "\n",
    "        #save loss states for graphing \n",
    "        if ((idx + 1) % 4000) == 0:\n",
    "            loss_states.append(train_loss)\n",
    "            print(train_log_string % (datetime.now(), epoch, idx + 1, len(train), np.mean(train_loss)))\n",
    "            train_loss = []\n",
    "\n",
    "\n",
    "    # Record at every epoch\n",
    "    print ('Train Loss at epoch{}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
    "    train_loss_record.append(np.mean(train_loss_sum))\n",
    "\n",
    "    # Valid\n",
    "    print ('Epoch{} Validating...'.format(epoch))\n",
    "\n",
    "    # loss\n",
    "    valid_loss = []\n",
    "    # dataloader\n",
    "    valid_dataloader = DataLoader(dataset=validDS, shuffle=True, num_workers=2, batch_size=1)\n",
    "\n",
    "    for idx, data in enumerate(valid_dataloader, 0):\n",
    "        # get data\n",
    "        s1, s2, label = data\n",
    "\n",
    "        # input\n",
    "        output = siamese(s1, s2)\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(output, Variable(label))\n",
    "        valid_loss.append(loss.data.cpu())\n",
    "\n",
    "    print(valid_log_string % (datetime.now(), epoch, np.mean(valid_loss)))\n",
    "    # Record\n",
    "    valid_loss_record.append(np.mean(valid_loss))\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    # Keep track of best record\n",
    "    if np.mean(valid_loss) < best_record:\n",
    "        best_record = np.mean(valid_loss)\n",
    "        # save the best model\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'siamese': siamese.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'loss_states': loss_states\n",
    "        }\n",
    "        torch.save(state_dict, 'Sgd4Best.pt')\n",
    "        print ('Model improved!\\n')\n",
    "\n",
    "    # save the longest running model\n",
    "    state_dict = {\n",
    "        'epoch': epoch,\n",
    "        'siamese': siamese.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'loss_states': loss_states\n",
    "    }\n",
    "    torch.save(state_dict, 'Sgd4Long.pt')\n",
    "    print ('Model saved!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "1. Using a softmax function to get out the prediction from our model, since the output is length 2\n",
    "2. Since the predictions are a continuous value, they are mapped to either 0 or 1 based on if they are less that .5 or not\n",
    "3. Next all true/false positive/negatives are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing \"\"\"\n",
    "\n",
    "#load testing data\n",
    "dL = DataLoader(dataset=testDS, shuffle=True, num_workers=2, batch_size=1)\n",
    "#using softmax to get prediction from the output\n",
    "sm = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "#save values to use for precision and recall\n",
    "true_pos = 0\n",
    "true_neg = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "\n",
    "#test\n",
    "for idx, data in enumerate(dL, 0):\n",
    "    # get data\n",
    "    s1, s2, label = data\n",
    "\n",
    "    # input\n",
    "    output = siamese(s2, s1)\n",
    "    output = output.squeeze(0)\n",
    "    res = sm(output.data)[:,1]\n",
    "    \n",
    "    predict = None\n",
    "    if res < .5:\n",
    "        predict = 0\n",
    "    else:\n",
    "        predict = 1\n",
    "        \n",
    "    if label == 1 and predict == 1:\n",
    "        true_pos += 1\n",
    "    elif label == 0 and predict == 1:\n",
    "        false_pos += 1\n",
    "    elif label == 1 and predict == 0:\n",
    "        false_neg += 1\n",
    "    elif label == 0 and predict == 0:\n",
    "        true_neg += 1\n",
    "    else: \n",
    "        print(\"Not a valid prediction/label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "1. Displayed below are results at different epochs and a graph of the loss states for the indicated epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = true_pos/(true_pos + false_neg)\n",
    "precision = true_pos/(true_pos + false_pos)\n",
    "accuracy = (true_pos + true_neg)/len(testDS)\n",
    "f1 = 2 * ((precision * recall)/(precision + recall))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"F1 score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_means = [np.mean(loss_states[i]) for i in range(len(loss_states))]\n",
    "x = [i for i in range(len(loss_states))]\n",
    "\n",
    "plt.plot(x, loss_means)\n",
    "plt.title(\"Mean training loss over length of \" + str(epoch - 1) + \" epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
